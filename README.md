# Interview phrase tagger
It provides a set of functionalities to tagger interviews according to main topics provided. `iptag` is a toolkit and CLI for **tagging/underlining phrases in interviews** according to a **configurable set of topics** (10, 30, 50+), using **offline/local NLP models** to preserve privacy and meet data-governance constraints.

The main goals are:

- Receive **raw interview transcripts** (e.g. `.txt`, `.jsonl`).
- **Segment** them into phrases or spans.
- **Assign one or more topics** to each phrase (multi-label).
- Optionally **export underlined/annotated text** in different formats (plain text, JSON, etc.).
- Run **fully offline** with CPU-friendly models.

---

## Features

- ðŸ” **Phrase-level tagging** of interview text (multi-label).
- ðŸ·ï¸ **Configurable topic sets** through YAML/JSON schemas.
- ðŸ”’ **Offline-first / local models** for privacy-sensitive data.
- ðŸ§© Clear **pipeline structure**: segmentation â†’ encoding â†’ classification â†’ export.
- ðŸ§ª Built-in **evaluation** utilities for multi-label settings.
- ðŸ’» Simple **CLI**: `iptag`.

> **Status:** early prototype / WIP.

---

## Installation

The next steps being executed in the terminal, and you have to be in the source folder of this repo.
1. Install [uv](https://github.com/astral-sh/uv). 
```bash
curl -Ls https://astral.sh/uv/install.sh | sh
```
2. Create a virtual environment in which you will install all the dependencies required in the project: 
```bash 
uv venv .venv
```
3. Activate the virtual environment 
```bash 
source .venv/bin/activate
```
4. Install the project to be able to execute the custom command line: 
```bash 
uv pip install -e .
```
5. Test that everything is working 
```bash 
uv run iptag
``` 
or 
```bash 
iptag
```
6. If you want to deactivate the environment just execute:
```bash 
deactivate
```

---

## Tests

### Markers
If you want to see all the available markers, just run:
```bash
uv run pytest --markers
```

#### Adding Markers to Tests
In your test functions, add markers using the `@pytest.mark.markername` decorator:

```python
@pytest.mark.unit
def test_fast_unit_test():
    """A fast unit test."""
    pass

@pytest.mark.integration
def test_component_integration():
    """Test integration between components."""
    pass

@pytest.mark.slow
def test_time_consuming_operation():
    """A test that takes a long time."""
    pass

@pytest.mark.slow
@pytest.mark.integration
def test_slow_integration():
    """A test with multiple markers."""
    pass
```

#### Basic Marker Usage
```bash
# Run only unit tests
uv run pytest -m unit

# Run only integration tests  
uv run pytest -m integration

# Run only slow tests
uv run pytest -m slow

# Skip slow tests
uv run pytest -m "not slow"

# Run unit OR integration tests
uv run pytest -m "unit or integration"

# Run integration AND slow tests
uv run pytest -m "integration and slow"

# Run unit tests but not slow ones
uv run pytest -m "unit and not slow"
```

---

## Project structure

Planned layout (subject to change):

```bash
interview-phrase-tagger/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml           # common config (paths, logging, etc.)
â”‚   â”œâ”€â”€ model_local.yaml    # config for offline local model
â”‚   â””â”€â”€ topics_example.yaml # example topic schema
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # raw interviews (text, json, etc.)
â”‚   â”œâ”€â”€ processed/          # cleaned+segmented phrases
â”‚   â””â”€â”€ annotations/        # human-labeled data
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ cli_usage.md
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ examples.md
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ checkpoints/        # saved model weights
â”‚   â””â”€â”€ artifacts/          # tokenizer, label encoders, etc.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.ipynb
â”‚   â”œâ”€â”€ 02_training.ipynb
â”‚   â””â”€â”€ 03_evaluation.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ eval_model.py
â”‚   â””â”€â”€ export_cli_bundle.py
â”œâ”€â”€ src/
â”‚   â””â”€â”€ iptag/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ settings.py
â”‚       â”œâ”€â”€ cli/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ annotate/
â”‚       â”‚   â”œâ”€â”€ data/
â”‚       â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logging_utils.py
â”‚       â”œâ”€â”€ phrase_segmentation/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ segmenter.py
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ classifier.py     # multi-label classifier wrapper
â”‚       â”‚   â””â”€â”€ loaders.py        # load local/offline models
â”‚       â”œâ”€â”€ topics/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ schema.py         # dynamic topic definitions
â”‚       â”œâ”€â”€ pipeline/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ tagging.py        # end-to-end pipeline
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ io.py
â”‚           â””â”€â”€ text_cleaning.py
â””â”€â”€ tests/
